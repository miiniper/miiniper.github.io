---
layout:     post
title:      k8s
subtitle:   k8s
date:       2019-01-16
author:     hanlei
header-img: img/bg-k8s.jpg
catalog: true
tags:
    - k8s
---

[TOC]



# k8s

## 常用命令

```
自动补全
source <(kubectl completion bash)
source <(kubectl completion zsh)

kubectl get node  -o wide
kubectl get pods   -o wide
kubectl get services -o wide
kubectl get deployments   -o wide
kubectl get namespaces
kubectl get nodes --show-labels
kubectl get ns
kubectl get pod -n kube-system

kubectl cluster-info
kubectl describe pod

驱逐
kubectl drain <node>   （daemonset 除外）
禁止调度
kubectl cordon <node>
kubectl uncordon <node>

打标签
kubectl label nodes 10.21.8.91 disktype=ssd
kubectl get nodes --show-labels
kubectl label nodes 10.21.8.91 disktype-         #删除
kubectl get pods -l environment=production,tier=frontend

污点
kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1:NoSchedule-
kubectl describe nodes node1

通过文件删除pod
kubectl delete -f  www.yml

```





## coredns

`https://github.com/coredns/deployment/tree/master/kubernetes`

`https://coredns.io/manual/toc/#configuration`



```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          upstream
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 60
        reload
        loadbalance
    }
    xxx.xxx.xxx.com {     #特殊的解析
        forward . 1.1.1.1 2.2.2.2
        log
    }
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: "CoreDNS"
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      nodeSelector:
        #traefik: "svc"
        type: traefik
        #coredns: "dns"
      serviceAccountName: coredns
      tolerations:
      - key: "CriticalAddonsOnly"
        operator: "Exists"
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: coredns
        image: coredns/coredns:1.2.0
        imagePullPolicy: IfNotPresent
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        - mountPath: /etc/localtime
          name: localtime
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 40
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 40
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3
      dnsPolicy: Default
      terminationGracePeriodSeconds: 90
      volumes:
      - name: config-volume
        configMap:
          name: coredns
          items:
          - key: Corefile
            path: Corefile
      - hostPath:
          path: /etc/localtime
          type: ""
        name: localtime
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 172.16.255.115
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
    
```

### configmap

configmap用于将应用所需的配置信息与程序进行分离, 使应用程序更好的复用, 在大规模容器集群环境中, 对应用进行统一配置管理.

使用ConfigMap有三种方式，一种是通过环境变量的方式，直接传递pod，另一种是通过在pod的命令行下运行的方式，第三种是使用volume的方式挂载入到pod内

configmap 不能跨namespace





### service account



什么是service account? 顾名思义，相对于user account（比如：kubectl访问APIServer时用的就是user account），service account就是Pod中的Process用于访问Kubernetes API的account，它为Pod中的Process提供了一种身份标识。相比于user account的全局性权限，service account更适合一些轻量级的task，更聚焦于授权给某些特定Pod中的Process所使用。

Kubernetes会为每个cluster中的namespace自动创建一个默认的service account资源，并命名为”default”.

这个资源会在控制类型的资源中用：例如

````
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: k8s-example1
spec:
  replicas: 1
  template:
    metadata:
      labels:
        run: k8s-example1
    spec:
      serviceAccount: k8s-example1      #引用serviceAccount
      containers:
      - name: k8s-example1
        image: k8s/example1:latest
        imagePullPolicy: IfNotPresent
````







### rbac 与角色控制

rbac 是api，由这个接口调用角色资源。

### Role与ClusterRole

在RBAC API中，一个角色包含了一套表示一组权限的规则。 权限以纯粹的累加形式累积（没有”否定”的规则）。 角色可以由命名空间（namespace）内的Role对象定义，而整个Kubernetes集群范围内有效的角色则通过ClusterRole对象实现。

一个Role对象只能用于授予对某一单一命名空间中资源的访问权限。 以下示例描述了”default”命名空间中的一个Role对象的定义，用于授予对pod的读访问权限：

```
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  namespace: default
  name: pod-reader
rules:

- apiGroups: [""] # 空字符串""表明使用core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```


ClusterRole对象可以授予与Role对象相同的权限，但由于它们属于集群范围对象， 也可以使用它们授予对以下几种资源的访问权限：

集群范围资源（例如节点，即node）
非资源类型endpoint（例如”/healthz”）
跨所有命名空间的命名空间范围资源（例如pod，需要运行命令kubectl get pods --all-namespaces来查询集群中所有的pod）
下面示例中的ClusterRole定义可用于授予用户对某一特定命名空间，或者所有命名空间中的secret（取决于其绑定方式）的读访问权限：

```
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  # 鉴于ClusterRole是集群范围对象，所以这里不需要定义"namespace"字段
  name: secret-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
```

### RoleBinding与ClusterRoleBinding

角色绑定将一个角色中定义的各种权限授予一个或者一组用户。 角色绑定包含了一组相关主体（即subject, 包括用户——User、用户组——Group、或者服务账户——Service Account）以及对被授予角色的引用。 在命名空间中可以通过`RoleBinding`对象授予权限，而集群范围的权限授予则通过`ClusterRoleBinding`对象完成。

`RoleBinding`可以引用在同一命名空间内定义的`Role`对象。 下面示例中定义的`RoleBinding`对象在”default”命名空间中将”pod-reader”角色授予用户”jane”。 这一授权将允许用户”jane”从”default”命名空间中读取pod。

```
# 以下角色绑定定义将允许用户"jane"从"default"命名空间中读取pod。
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

最后，可以使用`ClusterRoleBinding`在集群级别和所有命名空间中授予权限。下面示例中所定义的`ClusterRoleBinding` 允许在用户组”manager”中的任何用户都可以读取集群中任何命名空间中的secret。

```
# 以下`ClusterRoleBinding`对象允许在用户组"manager"中的任何用户都可以读取集群中任何命名空间中的secret。
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
  
```



## jaeger

`jaeger github   https://github.com/jaegertracing/jaeger`

`jaeger documenttation    https://www.jaegertracing.io/docs/1.8/`

`jaeger k8s templates   https://github.com/jaegertracing/jaeger-kubernetes`

jaeger用于监视和排除基于微服务的分布式系统，包括：

分布式上下文传播，分布式事务监控，根本原因分析，服务依赖性分析，性能/延迟优化



### ingress controller 

Ingress Controller是一个统称,并不只有一个,如下

Ingress NGINX: Kubernetes 官方维护的方案,也是本次安装使用的 Controller。
F5 BIG-IP Controller: F5 所开发的 Controller,它能够让管理员通过 CLI 或 API 让 Kubernetes 与 OpenShift 管理 F5 BIG-IP 设备。
Ingress Kong: 著名的开源 API Gateway 方案所维护的 Kubernetes Ingress Controller。
Traefik: 是一套开源的 HTTP 反向代理与负载均衡器,而它也支援了 Ingress。
Voyager: 一套以 HAProxy 为底的 Ingress Controller。

`https://github.com/containous/traefik/tree/master/examples/k8s`

`https://github.com/containous/traefik/blob/b6498cdcbc373f4a99984ef531c931e18a85545c/docs/user-guide/kubernetes.md`



## filebeat收集k8s日志

```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: kube-system
  labels:
    k8s-app: filebeat
data:
  filebeat.yml: |-
    filebeat.config:
      inputs:
        path: ${path.config}/inputs.d/*.yml
        reload.enabled: false
      modules:
        path: ${path.config}/modules.d/*.yml
        reload.enabled: false

      filebeat.prospectors:
      - input_type: log
        # 探矿者在指定的路径中检查新文件的频率
        # 而不会导致Filebeat过于频繁地扫描。默认值：10s。
        scan_frequency: 10s
        # 定义每个采集器在获取文件时使用的缓冲区大小
        harvester_buffer_size: 32768
        #单个日志事件可以拥有的最大字节数
        #max_bytes之后的所有字节被丢弃并且不发送默认值是10MB
        #这对于可能变大的多行日志消息特别有用
        max_bytes: 10485760
        #enabled: true
        encoding: utf-8
        #tail_files: true
        #ignore_older: 7d
        fields_under_root: true
        json.keys_under_root: true
        json.add_error_key: true
        json.message_key: log
      - decode_json_fields:
        fields: ["log"]
        process_array: false
        max_depth: 3
        target: ""
        overwrite_keys: false
        paths:
        - /var/lib/docker/containers/*/*-json.log
    output.kafka:
      enabled: true
      hosts: ["kafka01.com:9092","kafka02.com:9092"]
      topic: 'a-%{[kubernetes.namespace]}'
      worker: 4
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-inputs
  namespace: kube-system
  labels:
    k8s-app: filebeat
data:
  kubernetes.yml: |-
    - type: docker
      containers.ids:
      - "*"
      processors:
        - add_kubernetes_metadata:
            in_cluster: true
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: kube-system
  labels:
    k8s-app: filebeat
spec:
  template:
    metadata:
      labels:
        k8s-app: filebeat
    spec:
      serviceAccountName: filebeat
      terminationGracePeriodSeconds: 30
      containers:
      - name: filebeat
        image: filebeat:6.3.1
        args: [
          "-c", "/etc/filebeat.yml",
          "-e",
        ]
        env:
        - name: ELASTICSEARCH_HOST
          value: ""
        - name: ELASTICSEARCH_PORT
          value: ""
        - name: ELASTICSEARCH_USERNAME
          value: elastic
        - name: ELASTICSEARCH_PASSWORD
          value: changeme
        - name: ELASTIC_CLOUD_ID
          value:
        - name: ELASTIC_CLOUD_AUTH
          value:
        securityContext:
          runAsUser: 0
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/filebeat.yml
          readOnly: true
          subPath: filebeat.yml
        - name: inputs
          mountPath: /usr/share/filebeat/inputs.d
          readOnly: true
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - mountPath: /etc/localtime
          name: localtime
          readOnly: true
      volumes:
      - name: config
        configMap:
          defaultMode: 0600
          name: filebeat-config
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: inputs
        configMap:
          defaultMode: 0600
          name: filebeat-inputs
      - hostPath:
          path: /etc/localtime
          type: ""
        name: localtime
      # We set an `emptyDir` here to ensure the manifest will deploy correctly.
      # It's recommended to change this to a `hostPath` folder, to ensure internal data
      # files survive pod changes (ie: version upgrade)
      #- name: data
      #  emptyDir: {}
      - name: data
        hostPath:
          path: /data/ifengsite
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: filebeat
subjects:
- kind: ServiceAccount
  name: filebeat
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: filebeat
  labels:
    k8s-app: filebeat
rules:
- apiGroups: [""] # "" indicates the core API group
  resources:
  - namespaces
  - pods
  verbs:
  - get
  - watch
  - list
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  namespace: kube-system
  labels:
    k8s-app: filebeat
---

```



## helm

### 概念

**Helm 有两个重要的概念：chart 和 release。**

 chart 是创建一个应用的信息集合，包括各种 Kubernetes 对象的配置模板、参数定义、依赖关系、文档说明等。chart 是应用部署的自包含逻辑单元。可以将 chart 想象成 apt、yum 中的软件安装包。
 release 是 chart 的运行实例，代表了一个正在运行的应用。当 chart 被安装到 Kubernetes 集群，就生成一个 release。chart 能够多次安装到同一个集群，每次安装都是一个 release。



Helm 是包管理工具，这里的包就是指的 chart。Helm 能够：

1. 从零创建新 chart。
2. 与存储 chart 的仓库交互，拉取、保存和更新 chart。
3. 在 Kubernetes 集群中安装和卸载 release。
4. 更新、回滚和测试 release。

Helm 包含两个组件：Helm 客户端 和 Tiller 服务器。


Helm 客户端是终端用户使用的命令行工具，用户可以：

1. 在本地开发 chart。
2. 管理 chart 仓库。
3. 与 Tiller 服务器交互。
4. 在远程 Kubernetes 集群上安装 chart。
5. 查看 release 信息。
6. 升级或卸载已有的 release。

Tiller 服务器运行在 Kubernetes 集群中，它会处理 Helm 客户端的请求，与 Kubernetes API Server 交互。Tiller 服务器负责：

1. 监听来自 Helm 客户端的请求。
2. 通过 chart 构建 release。
3. 在 Kubernetes 中安装 chart，并跟踪 release 的状态。
4. 通过 API Server 升级或卸载已有的 release。

简单的讲：Helm 客户端负责管理 chart；Tiller 服务器负责管理 release。

[helm官方文档](https://docs.helm.sh/)                       

[helm中文文档](https://whmzsu.github.io/helm-doc-zh-cn/)

### 安装

下载 https://github.com/helm/helm/releases/tag/v2.12.2 ，解压直接将helm命令移动到path中。

创建tiller的`serviceaccount`和`clusterrolebinding`

```bash
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
```

```bash
helm init --service-account tiller   （有坑）
```

由于网络缘故无法拉去原本的镜像，提供两种解决办法

1.目前最新版v2.12.2，可以使用阿里云镜像，如： 
`helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.8.2 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts）`

我们使用-i指定自己的镜像，因为官方的镜像因为某些原因无法拉取，
官方镜像地址是：gcr.io/kubernetes-helm/tiller:v2.12.2，
本地存放镜像，版本与helm客户端的版本相同，使用helm version可查看helm客户端版本。

2. 自己创建/root/.helm/repository/repositories.yaml

   ```
   apiVersion: v1
   repositories:
     - name: charts
       url: "https://kubernetes-charts.storage.googleapis.com"
     - name: ali
       url: "https://xxx.xxx.com/file/tiller:v2.12.2"
   ```

   执行helm init --service-account tiller  ,可以看到tiller的svc，deplement ,pod已经创建但是pod镜像一直拉取失败，执行以下：

   ```
   kubectl get deployment -n kube-system tiller-deploy  -o yaml > tiller-deploy.yaml
   kubectl delete  deployment -n kube-system tiller-deploy
   vim tiller-deploy.yaml 
   修改
   Deployment.spec.template.spec.image  为本地镜像。
   
   kubectl apply -f tiller-deploy.yaml
   ```

   ```
   [root@VM_8_3_centos repository]# helm version 
   Client: &version.Version{SemVer:"v2.12.2", GitCommit:"7d2b0c73d734f6586ed222a567c5d103fed435be", GitTreeState:"clean"}
   Server: &version.Version{SemVer:"v2.12.2", GitCommit:"7d2b0c73d734f6586ed222a567c5d103fed435be", GitTreeState:"clean"}
   [root@VM_8_3_centos repository]# 
   ```

   

   **helm 补全**

   编辑.bashrc,追加 source <(helm completion bash)

   ### 常用命令

   - helm repo list
   - helm search mysql
   - helm  repo update
   - helm list
   - helm delete xxxx -force
   - helm install ./prometheus --name prometheus --namespace=monitoring
   - helm upgrade -f kube-prometheus/values.yaml prometheus ./kube-prometheus/
   - helm delete prometheus --purge

   ### helm安装方式

   - 指定chart: helm install stable/mariadb
   - 指定打包的chart: helm install ./nginx-1.2.3.tgz 
   - 指定打包目录: helm install ./prometheus --name prometheus --namespace=monitoring
   - 指定chart包URL: helm install https://example.com/charts/nginx-1.2.3.tgz

   **CHART chart 是 Helm 的应用打包格式，一旦安装了某个 chart，我们就可以在 ~/.helm/cache/archive 中找到 chart 的 tar 包。**

### chart目录结构

[chart目录结构详细介绍](https://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653588705&idx=1&sn=5d960e522fa40ffbb3f622679f3d1392&chksm=8d3084f8ba470deefcc11fd247c6a6bbf015cfe85063cf6a7c6ca41a41fbd6b40ff172e3ca28&scene=21#wechat_redirect)


- **Chart.yaml** YAML 文件，描述 chart 的概要信息。name 和 version 是必填项，其他都是可选。
- **README.md** Markdown 格式的 README 文件
- **values.yaml** chart 支持在安装的时根据参数进行定制化配置，而 values.yaml 则提供了这些配置参数的默认值。
- **templates目录**  各类 Kubernetes 资源的配置模板都放置在这里。Helm 会将 values.yaml 中的参数值注入到模板中生成标准的 YAML 配置文件。模板是 chart 最重要的部分，也是 Helm 最强大的地方。模板增加了应用部署的灵活性，能够适用不同的环境，我们后面会详细讨论。
- **templates/NOTES.txt** chart 的简易使用文档，chart 安装成功后会显示此文档内容。与模板一样，可以在 NOTE.txt 中插入配置参数，Helm 会动态注入参数值。

### 更新  helm upgrade  

修改了values.yaml文件，upgrade后 prometheus-kube-prometheus-0会重启  数据会被清空 ，后续考虑共享存储

```bash
helm upgrade -f kube-prometheus/values.yaml kube-prometheus ./kube-prometheus/
```

### 版本回滚

```
helm rollback prometheus 3
```